{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Step 1: Extract and Load Pre-collected Data\n",
    "\n",
    "Since we don't have access to the OpenTouch Interface on Mac, we'll use\n",
    "pre-collected touch sensor data that has already been captured.\n",
    "\n",
    "* The coin_data.zip file contains organized PNG images from different coin types.\n",
    "* Each folder represents a different class/label (e.g., one_euro, two_euro, etc.).\n",
    "* We'll extract this data and inspect its structure.\n",
    "\n",
    "Note: This data was originally collected using DIGIT touch sensors on Ubuntu.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "zip_path = \"coin_data.zip\"\n",
    "extract_dir = Path(os.path.splitext(zip_path)[0] + \"_extracted\")\n",
    "\n",
    "# Create extraction directory\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Extracting {zip_path} into {extract_dir}...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Check what was extracted\n",
    "if extract_dir.exists():\n",
    "    print(f\"\\nSuccessfully extracted dataset to: {extract_dir}\")\n",
    "\n",
    "    # List the coin types (subdirectories)\n",
    "    coin_types = [d.name for d in extract_dir.iterdir() if d.is_dir()]\n",
    "    coin_types.sort()\n",
    "\n",
    "    print(f\"Found {len(coin_types)} coin types:\")\n",
    "    for coin_type in coin_types:\n",
    "        coin_dir = extract_dir / coin_type\n",
    "        num_images = len(list(coin_dir.glob(\"*.png\")))\n",
    "        print(f\"  - {coin_type}: {num_images} images\")\n",
    "else:\n",
    "    print(f\"ERROR: {extract_dir} directory not found after extraction!\")\n",
    "    print(f\"Make sure {zip_path} is in your working directory.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 2: Load Images for One Label\n",
    "\n",
    "Load the images from one coin type folder into a camera_data list.\n",
    "This simulates what would happen when loading data from a single .touch file.\n",
    "\n",
    "* Choose which coin type folder to process (e.g., \"two_euro\", \"one_euro\").\n",
    "* Load all images from that folder into a list of numpy arrays.\n",
    "* This creates the same `camera_data` variable as in the sensor data workflow.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO: Choose which label to process\n",
    "label_name = \"two_euro\"  # Change this to: \"one_euro\", \"ten_cent\", \"no_touch\", etc.\n",
    "\n",
    "# Path to the extracted dataset\n",
    "dataset_dir = Path(\"coin_data_extracted\")\n",
    "coin_path = dataset_dir / label_name\n",
    "\n",
    "if not coin_path.exists():\n",
    "    available = [d.name for d in dataset_dir.iterdir() if d.is_dir()]\n",
    "    raise FileNotFoundError(f\"Directory '{coin_path}' does not exist. Available folders: {available}\")\n",
    "\n",
    "# Get all PNG files and sort them\n",
    "image_files = sorted([f for f in os.listdir(coin_path) if f.lower().endswith('.png')])\n",
    "\n",
    "# Load each image as a numpy array\n",
    "camera_data = []\n",
    "for img_file in image_files:\n",
    "    img_path = coin_path / img_file\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    camera_data.append(img_array)\n",
    "\n",
    "print(f\"Loaded {len(camera_data)} images from '{label_name}' folder\")\n",
    "if camera_data:\n",
    "    print(f\"Image shape: {camera_data[0].shape}\")\n"
   ],
   "id": "4a0e59ae1d3a58e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 3: Filter the Frames\n",
    "\n",
    "Each dataset should only contain images of its respective label.\n",
    "Remove frames that don't match (e.g., \"no touch\" images in a \"coin\" dataset).\n",
    "\n",
    "Why?\n",
    "The raw data also includes unwanted frames (like empty touches or noise).\n",
    "Filtering ensures that each dataset is clean and only contains the intended label.\n",
    "\n",
    "* Exception: If you are creating a \"no touch\" dataset, keep the empty frames.\n",
    "* Hint: You can do this both programmatically and using your file explorer.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "no_touch = camera_data[:20]  # Assume the first 20 images don't show any touch. Adjust as needed.\n",
    "avg_empty_image = np.mean(np.stack(no_touch, axis=0), axis=0)\n",
    "\n",
    "def mean_square_error(image_a: np.ndarray, image_b: np.ndarray) -> float:\n",
    "    diff = image_a - image_b\n",
    "    return np.mean(diff ** 2)\n",
    "\n",
    "threshold = 40.0  # <-- TODO: Adjust as needed\n",
    "# print(mean_square_error(avg_empty_image, camera_data[100]))\n",
    "\n",
    "with_touch = [frame for frame in camera_data if mean_square_error(frame, avg_empty_image) > threshold]\n",
    "# with_touch = camera_data  # Use this when having a dataset with no touch\n",
    "print(f'There are {len(with_touch)} images with recognized touch')"
   ],
   "id": "69bd47044eb45c96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 4: Save the Cleaned Dataset\n",
    "\n",
    "Now save the filtered frames to disk.\n",
    "\n",
    "* Saving them as `.png` files makes it easy to inspect the images in your file explorer.\n",
    "* Each dataset (per label) will be stored in its own folder.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Use the label name as the dataset name\n",
    "directory = os.path.join('coin_data', label_name)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f'Saving {len(with_touch)} images to {directory}/')\n",
    "for i, frame in enumerate(with_touch):\n",
    "    img = Image.fromarray(frame.astype(np.uint8))\n",
    "    img.save(os.path.join(directory, f'{label_name}_{i:04d}.png'))"
   ],
   "id": "bfa6076176e2b29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 5: Create a simple model to convert RGB images to grayscale.\n",
    "\n",
    "* The filter converts RGB images to grayscale using the standard luminosity method.\n",
    "* It inherits from `BaseFilter` and implements `forward` and `onnx_export`.\n",
    "* Finally, the model is saved to disk for later use.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rgb_to_grayscale(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert an RGB image (H, W, 3) uint8 to grayscale (H, W) uint8.\n",
    "    \"\"\"\n",
    "    r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "    grayscale = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return grayscale.astype(np.uint8)\n",
    "\n",
    "# Take the first image from the loaded dataset\n",
    "original = camera_data[0]   # shape (H, W, 3), dtype=uint8\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = rgb_to_grayscale(original)\n",
    "\n",
    "# Show both images\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original)\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "plt.title(\"Grayscale\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a3407cc050bd428a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 6: Define the CNN Model\n",
    "\n",
    "We now build a Convolutional Neural Network (CNN) to classify the coins.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class CoinClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Define the CNN architecture\n",
    "        self.model = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 320x240 -> 160x120\n",
    "\n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 160x120 -> 80x60\n",
    "\n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 80x60 -> 40x30\n",
    "\n",
    "            # Global average pooling and classifier\n",
    "            nn.AdaptiveAvgPool2d(1),  # 40x30 -> 1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass through the CNN\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess input images from [N, H, W, C] to [N, C, H, W].\n",
    "        Normalizes pixel values from [0, 255] to [0, 1].\n",
    "        \"\"\"\n",
    "        x = x.float() / 255.0\n",
    "        return x.permute(0, 3, 1, 2)\n"
   ],
   "id": "1655907662e928db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This is an alternative to step 6 where we use a pre-trained model from PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class CoinClassifierEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "\n",
    "        # Load pretrained EfficientNet-B4 backbone\n",
    "        weights = EfficientNet_B4_Weights.DEFAULT\n",
    "        backbone = efficientnet_b4(weights=weights)\n",
    "\n",
    "        # Freeze backbone parameters\n",
    "        for param in backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace classifier head\n",
    "        backbone.classifier[1] = nn.Linear(\n",
    "            backbone.classifier[1].in_features,\n",
    "            num_classes,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.model = backbone\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess input images from [N, H, W, C] uint8 [0, 255].\n",
    "        Converts to [N, C, 380, 380] float32 normalized for EfficientNet.\n",
    "        \"\"\"\n",
    "        x = x.float() / 255.0\n",
    "        x = x.permute(0, 3, 1, 2)  # [N, C, H, W]\n",
    "\n",
    "        inout_h, input_w = 240, 320\n",
    "        target_size = 380\n",
    "\n",
    "        # Scale while preserving aspect ratio\n",
    "        scale = min(target_size / inout_h, target_size / input_w)\n",
    "        new_h, new_w = int(inout_h * scale), int(input_w * scale)\n",
    "\n",
    "        x = torch.nn.functional.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Pad to square\n",
    "        pad_h = target_size - new_h\n",
    "        pad_w = target_size - new_w\n",
    "        pad_top = pad_h // 2\n",
    "        pad_left = pad_w // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        x = torch.nn.functional.pad(x, (pad_left, pad_right, pad_top, pad_bottom))\n",
    "\n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        return x\n"
   ],
   "id": "c88101ad20cb7b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 7: Load the Coin Datasets\n",
    "\n",
    "Now we load the saved images back into memory.\n",
    "\n",
    "* Each coin type should be in its own subdirectory under `coin_data/`.\n",
    "* A label mapping is created automatically from the folder names.\n",
    "* Optionally, datasets are balanced so all classes have the same number of images.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_coin_datasets(dset_path: str, balance_datasets = True, max_size=2000) -> tuple[np.ndarray, np.ndarray, dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load coin images from subdirectories\n",
    "\n",
    "    Expected structure:\n",
    "    coin_data/\n",
    "    ├── two_euro/\n",
    "    │   ├── two_euro_0001.png\n",
    "    │   └── ...\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dset_path):\n",
    "        raise FileNotFoundError(f\"Directory '{dset_path}' does not exist.\")\n",
    "\n",
    "    coin_dirs = [d for d in os.listdir(dset_path) if os.path.isdir(os.path.join(dset_path, d))]\n",
    "    coin_dirs.sort()\n",
    "\n",
    "    if not coin_dirs:\n",
    "        raise ValueError(f\"No subdirectories found in the directory '{dset_path}'.\")\n",
    "\n",
    "    # Create label mapping\n",
    "    labels = {j: coin_name for j, coin_name in enumerate(coin_dirs)}\n",
    "    print(f\"Found {len(coin_dirs)} coin types:\")\n",
    "    for label, name in labels.items():\n",
    "        print(f\"  Label {label}: {name}\")\n",
    "\n",
    "    # Load all images\n",
    "    coin_images = {}  # label -> list of images\n",
    "    for label, coin_name in labels.items():\n",
    "        coin_path = os.path.join(dset_path, coin_name)\n",
    "\n",
    "        all_files = os.listdir(coin_path)\n",
    "        image_files = [f for f in all_files if f.lower().endswith('.png')]\n",
    "        image_files.sort()\n",
    "\n",
    "        images = []\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(coin_path, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "        coin_images[label] = images\n",
    "\n",
    "    # Balance datasets\n",
    "    all_images, all_labels = [], []\n",
    "    if balance_datasets:\n",
    "        smallest_dset = min(min(len(images) for images in coin_images.values()), max_size)\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images[:smallest_dset])\n",
    "            all_labels.extend([label] * smallest_dset)\n",
    "    else:\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend([label] * len(images))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.stack(all_images, axis=0)  # Shape: (N, H, W, C)\n",
    "    Y = np.array(all_labels)  # Shape: (N,)\n",
    "\n",
    "    return X, Y, labels"
   ],
   "id": "f7f3446ced01a5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 8: Train the Model\n",
    "\n",
    "Now we bring everything together:\n",
    "\n",
    "* Load the datasets and convert them to PyTorch tensors.\n",
    "* Create a DataLoader for batching and shuffling.\n",
    "* Initialize and train the CNN.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load datasets\n",
    "X, y, label_mapping = load_coin_datasets(\"coin_data_extracted\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).to(torch.uint8)   # keep uint8 for preprocessing\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Device selection\n",
    "if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "    print(\"Using XPU (Intel Arc / oneAPI)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon / macOS GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create the model (choose one)\n",
    "num_classes = len(label_mapping)\n",
    "# model = CoinClassifier(num_classes=num_classes)\n",
    "model = CoinClassifierEfficientNet(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Move data to device\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Preprocess inputs\n",
    "        batch_x = model.preprocess(batch_x)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ],
   "id": "fc1f66aeb910b543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 9: Predict from a Single Image\n",
    "\n",
    "We now define a helper function to classify one image with the trained PyTorch model.\n",
    "\n",
    "* The image is expanded with a batch dimension before inference.\n",
    "* The model returns class scores (logits).\n",
    "* The highest-scoring class is mapped back to its label.\n",
    "\"\"\"\n",
    "\n",
    "def predict_coin(image: np.ndarray, model: torch.nn.Module, label_mapping: dict[int, str]) -> str:\n",
    "    \"\"\"Predict coin type from a single image.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure input is on the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    input_batch = torch.from_numpy(np.expand_dims(image, axis=0)).to(torch.uint8).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_batch = model.preprocess(input_batch)\n",
    "        output = model(input_batch)\n",
    "\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    predicted_label = label_mapping[predicted_class]\n",
    "\n",
    "    return predicted_label\n"
   ],
   "id": "82a1f3bb5c810393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 10: Test the Model\n",
    "\n",
    "Finally, let's check the trained model on a few random images.\n",
    "\n",
    "* Pick random samples from the dataset.\n",
    "* Run predictions with `predict_coin`.\n",
    "* Compare predicted vs. true labels.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(20):\n",
    "    idx = random.randint(0, len(X) - 1)\n",
    "    test_image = X[idx]\n",
    "    true_label = label_mapping[y[idx]]\n",
    "\n",
    "    predicted_label = predict_coin(test_image, model, label_mapping)\n",
    "    print(f\"True label: {true_label} | Predicted label: {predicted_label} | {true_label == predicted_label}\")\n"
   ],
   "id": "e3e5167d011b1297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 11: Evaluate Accuracy\n",
    "\n",
    "Instead of just printing individual results,\n",
    "we can calculate overall accuracy across 100 random images.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "num_tests = 100\n",
    "correct = 0\n",
    "\n",
    "for i in range(num_tests):\n",
    "    idx = random.randint(0, len(X) - 1)\n",
    "    test_image = X[idx]\n",
    "    true_label = label_mapping[y[idx]]\n",
    "    predicted_label = predict_coin(test_image, model, label_mapping)\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / num_tests\n",
    "print(f\"Accuracy over {num_tests} random samples: {accuracy:.2%}\")\n"
   ],
   "id": "f846caa074816932",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
