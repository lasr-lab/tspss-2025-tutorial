{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Step 1: Collect and Load Data\n",
    "\n",
    "Use the OpenTouch Interface to record your dataset.\n",
    "\n",
    "* Make one recording per data class (e.g., one for each type of coin).\n",
    "* Each `.touch` file should contain data for only one label/class.\n",
    "* You’ll need to repeat the loading and preprocessing steps for all .touch files (one per label) before training.\n",
    "\n",
    "Tip: Seeing a warning from Streamlit is normal and not an error.\n",
    "\"\"\"\n",
    "from opentouch_interface.decoder import Decoder\n",
    "\n",
    "# Expects the full path e.g. \"/home/someone/datasets/my_dataset.touch\"\n",
    "# Seeing a warning from Streamlit is normal\n",
    "path = ...\n",
    "\n",
    "dataset = Decoder(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 2: Inspect the Raw Data\n",
    "\n",
    "Now let’s check the structure of your recorded dataset.\n",
    "\n",
    "* `dataset.sensor_names` (list[str]) lists all sensors that were captured.\n",
    "* `dataset.stream_names_of(sensor_name)` (list[str]) lists the streams for a given sensor\n",
    "  (for DIGIT this will just be \"camera\").\n",
    "\"\"\"\n",
    "\n",
    "print(f'The following sensors have been captured: {dataset.sensor_names}')\n",
    "print(f'The sensors have the following streams:')\n",
    "for sensor in dataset.sensor_names:\n",
    "    print(f'\\t- {sensor}: {dataset.stream_names_of(sensor)}')"
   ],
   "id": "91beebec50b740ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 3: Grab the Camera Frames\n",
    "\n",
    "The raw dataset contains both the sensor data and additional metadata\n",
    "(e.g., timestamps). For training we only need the actual frames.\n",
    "\n",
    "* `dataset.stream_data_of(sensor_name, stream_name)` returns the list of frames.\n",
    "* For DIGIT, the stream is `\"camera\"`, which gives you the captured images.\n",
    "\n",
    "Hint: Call `dataset.stream_data_of` with `with_delta=False`.\n",
    "\"\"\"\n",
    "\n",
    "sensor_name = dataset.sensor_names[0]\n",
    "data_stream = 'camera'\n",
    "camera_data = dataset.stream_data_of(sensor_name, data_stream, with_delta=False)"
   ],
   "id": "8292e4266d5f30ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 4: Filter the Frames\n",
    "\n",
    "Each dataset should only contain images of its respective label.\n",
    "Remove frames that don’t match (e.g., \"no touch\" images in a \"coin\" dataset).\n",
    "\n",
    "Why?\n",
    "The raw data also includes unwanted frames (like empty touches or noise).\n",
    "Filtering ensures that each dataset is clean and only contains the intended label.\n",
    "\n",
    "* Exception: If you are creating a \"no touch\" dataset, keep the empty frames.\n",
    "* Hint: You can do this both programmatically and using your file explorer.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "no_touch = camera_data[:20]  # Assume the first 20 images don't show any touch. Adjust as needed.\n",
    "avg_empty_image = np.mean(np.stack(no_touch, axis=0), axis=0)\n",
    "\n",
    "def mean_square_error(image_a: np.ndarray, image_b: np.ndarray) -> float:\n",
    "    diff = image_a - image_b\n",
    "    return np.mean(diff ** 2)\n",
    "\n",
    "threshold = 40.0  # <-- TODO: Adjust as needed\n",
    "# print(mean_square_error(avg_empty_image, camera_data[100]))\n",
    "\n",
    "with_touch = [frame for frame in camera_data if mean_square_error(frame, avg_empty_image) > threshold]\n",
    "# with_touch = camera_data  # Use this when having a dataset with no touch\n",
    "print(f'There are {len(with_touch)} images with recognized touch')"
   ],
   "id": "d88b3a00398c1038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 5: Save the Cleaned Dataset\n",
    "\n",
    "Now save the filtered frames to disk.\n",
    "\n",
    "* Saving them as `.png` files makes it easy to inspect the images in your file explorer.\n",
    "* Each dataset (per label) will be stored in its own folder.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dset_name = os.path.splitext(os.path.basename(path))[0]\n",
    "directory = os.path.join('coin_data', dset_name)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f'Saving {len(with_touch)} images to {directory}/')\n",
    "for i, frame in enumerate(with_touch):\n",
    "    img = Image.fromarray(frame.astype(np.uint8))\n",
    "    img.save(os.path.join(directory, f'{dset_name}_{i:04d}.png'))"
   ],
   "id": "7b07b505a87336cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Create a simple model to convert RGB images to grayscale.\n",
    "\n",
    "* The filter converts RGB images to grayscale using the standard luminosity method.\n",
    "* It inherits from `BaseFilter` and implements `forward` and `onnx_export`.\n",
    "* Finally, the model is saved to disk for later use.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch.onnx\n",
    "\n",
    "from opentouch.core.base_filter import BaseFilter\n",
    "\n",
    "\n",
    "class GrayscaleFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    A simple model that converts an input image tensor to a grayscale image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height: int, width: int):\n",
    "        super().__init__()\n",
    "        self.height: int = height\n",
    "        self.width: int = width\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"A model that converts an input RGB image to a grayscale image.\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to convert the input tensor to grayscale.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, H, W, C) from CV2, where C is expected to be 3 (RGB channels).\n",
    "        Returns:\n",
    "            torch.Tensor: Grayscale image tensor of shape (N, 1, H, W).\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert from (N, H, W, C) to (N, C, H, W) format for PyTorch processing\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Convert to float if input is uint8\n",
    "        if x.dtype == torch.uint8:\n",
    "            x = x.float() / 255.0\n",
    "\n",
    "        # Convert RGB to grayscale using the standard luminosity method\n",
    "        # Grayscale = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
    "        r, g, b = x[:, 0:1, :, :], x[:, 1:2, :, :], x[:, 2:3, :, :]\n",
    "        grayscale = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "        return grayscale\n",
    "\n",
    "\n",
    "    def onnx_export(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Constructs the parameters needed for torch.onnx.export().\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            'example_input': torch.randint(0, 256, (1, 320, 240, 3), dtype=torch.uint8),\n",
    "            'input_names': ['input'],\n",
    "            'output_names': ['output'],\n",
    "        }\n",
    "\n",
    "# Saving the model (if not already saved)\n",
    "gray_filter = GrayscaleFilter(height=320, width=240)\n",
    "gray_filter.save(\"grayscale_filter\")"
   ],
   "id": "72ec1324494ec70d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 6: Define the CNN Model\n",
    "\n",
    "We now build a Convolutional Neural Network (CNN) to classify the coins.\n",
    "\n",
    "* The model takes RGB frames as input (3 channels).\n",
    "* It outputs one class per label in your dataset.\n",
    "* Preprocessing converts images from [N, H, W, C] to [N, C, H, W]\n",
    "  and normalizes pixel values to [0, 1].\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from opentouch.core.base_cnn import BaseCNN\n",
    "\n",
    "\n",
    "class CoinClassifier(BaseCNN):\n",
    "    def __init__(self, label_mapping: dict) -> None:\n",
    "        super().__init__(input_channels=3, label_mapping=label_mapping)\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        labels = ', '.join(self.label_mapping.values())\n",
    "        return f\"A CNN classifier for distinguishing between: {labels}\"\n",
    "\n",
    "    def build(self) -> None:\n",
    "        \"\"\"CNN architecture for coin classification\"\"\"\n",
    "        self.model = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 320x240 -> 160x120\n",
    "\n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 160x120 -> 80x60\n",
    "\n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 80x60 -> 40x30\n",
    "\n",
    "            # Global average pooling and classifier\n",
    "            nn.AdaptiveAvgPool2d(1),  # 40x30 -> 1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def preprocess(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess input images from [N, H, W, C] to [N, C, H, W] format.\n",
    "        Normalizes pixel values from [0, 255] to [0, 1].\n",
    "        \"\"\"\n",
    "        x = x.float() / 255.0\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "    def onnx_export(self) -> Dict[str, Any]:\n",
    "        \"\"\"Defines parameters needed for ONNX export.\"\"\"\n",
    "        return {\n",
    "            'example_input': torch.randint(0, 256, (1, 320, 240, 3), dtype=torch.uint8),\n",
    "            'input_names': ['input'],\n",
    "            'output_names': ['output'],\n",
    "        }"
   ],
   "id": "98c4b94e1c28063b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This is an alternative to step 6 where we use a pre-trained model from PyTorch.\n",
    "\"\"\"\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from opentouch.core.base_cnn import BaseCNN\n",
    "from opentouch.core.model_loader import ModelLoader\n",
    "\n",
    "\n",
    "class CoinClassifierEfficientNet(BaseCNN):\n",
    "    def __init__(self, label_mapping: dict) -> None:\n",
    "        super().__init__(input_channels=3, label_mapping=label_mapping)\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        labels = ', '.join(self.label_mapping.values())\n",
    "        return f\"EfficientNet-B4 transfer learning classifier for: {labels}\"\n",
    "\n",
    "    def build(self) -> None:\n",
    "        \"\"\"Build EfficientNet with frozen backbone, only train classifier\"\"\"\n",
    "        from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "\n",
    "        weights = EfficientNet_B4_Weights.DEFAULT\n",
    "        backbone = efficientnet_b4(weights=weights)\n",
    "\n",
    "        # Freeze ALL backbone parameters - only train the classifier\n",
    "        for param in backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace classifier head\n",
    "        backbone.classifier[1] = nn.Linear(\n",
    "            backbone.classifier[1].in_features,\n",
    "            self.num_classes,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.model = backbone\n",
    "\n",
    "    def preprocess(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        EfficientNet preprocessing with aspect ratio preservation\n",
    "        Input: [N, H, W, C] uint8 [0, 255]\n",
    "        Output: [N, C, H, W] float32 normalized\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.float() / 255.0  # Convert to float and normalize to [0, 1]\n",
    "        x = x.permute(0, 3, 1, 2)  # Permute to [N, C, H, W]\n",
    "\n",
    "        # Resize to EfficientNet input size (380x380 for B4) with aspect ratio preservation\n",
    "        inout_h, input_w = 240, 320\n",
    "        target_size = 380\n",
    "\n",
    "        # Calculate scale to fit within target size\n",
    "        scale = min(target_size / inout_h, target_size / input_w)\n",
    "        new_h, new_w = int(inout_h * scale), int(input_w * scale)\n",
    "\n",
    "        # Resize maintaining aspect ratio\n",
    "        x = torch.nn.functional.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Pad to square (center the image)\n",
    "        pad_h = target_size - new_h\n",
    "        pad_w = target_size - new_w\n",
    "        pad_top = pad_h // 2\n",
    "        pad_left = pad_w // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        x = torch.nn.functional.pad(x, (pad_left, pad_right, pad_top, pad_bottom))\n",
    "\n",
    "        # ImageNet normalization (what EfficientNet expects)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        return x\n",
    "\n",
    "    def onnx_export(self) -> Dict[str, Any]:\n",
    "        \"\"\"Defines parameters needed for ONNX export.\"\"\"\n",
    "        return {\n",
    "            'example_input': torch.randint(0, 256, (1, 320, 240, 3), dtype=torch.uint8),\n",
    "            'input_names': ['input'],\n",
    "            'output_names': ['output'],\n",
    "        }"
   ],
   "id": "7f3b5c0ac5a688bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 7: Load the Coin Datasets\n",
    "\n",
    "Now we load the saved images back into memory.\n",
    "\n",
    "* Each coin type should be in its own subdirectory under `coin_data/`.\n",
    "* A label mapping is created automatically from the folder names.\n",
    "* Optionally, datasets are balanced so all classes have the same number of images.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_coin_datasets(dset_path: str, balance_datasets = True, max_size=2000) -> tuple[np.ndarray, np.ndarray, dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load coin images from subdirectories\n",
    "\n",
    "    Expected structure:\n",
    "    coin_data/\n",
    "    ├── two_euro/\n",
    "    │   ├── two_euro_0001.png\n",
    "    │   └── ...\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dset_path):\n",
    "        raise FileNotFoundError(f\"Directory '{dset_path}' does not exist.\")\n",
    "\n",
    "    coin_dirs = [d for d in os.listdir(dset_path) if os.path.isdir(os.path.join(dset_path, d))]\n",
    "    coin_dirs.sort()\n",
    "\n",
    "    if not coin_dirs:\n",
    "        raise ValueError(f\"No subdirectories found in the directory '{dset_path}'.\")\n",
    "\n",
    "    # Create label mapping\n",
    "    labels = {j: coin_name for j, coin_name in enumerate(coin_dirs)}\n",
    "    print(f\"Found {len(coin_dirs)} coin types:\")\n",
    "    for label, name in labels.items():\n",
    "        print(f\"  Label {label}: {name}\")\n",
    "\n",
    "    # Load all images\n",
    "    coin_images = {}  # label -> list of images\n",
    "    for label, coin_name in labels.items():\n",
    "        coin_path = os.path.join(dset_path, coin_name)\n",
    "\n",
    "        all_files = os.listdir(coin_path)\n",
    "        image_files = [f for f in all_files if f.lower().endswith('.png')]\n",
    "        image_files.sort()\n",
    "\n",
    "        images = []\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(coin_path, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "        coin_images[label] = images\n",
    "\n",
    "    # Balance datasets\n",
    "    all_images, all_labels = [], []\n",
    "    if balance_datasets:\n",
    "        smallest_dset = min(min(len(images) for images in coin_images.values()), max_size)\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images[:smallest_dset])\n",
    "            all_labels.extend([label] * smallest_dset)\n",
    "    else:\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend([label] * len(images))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.stack(all_images, axis=0)  # Shape: (N, H, W, C)\n",
    "    Y = np.array(all_labels)  # Shape: (N,)\n",
    "\n",
    "    return X, Y, labels"
   ],
   "id": "f958e910ee30b60a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 8: Train and Save the Model\n",
    "\n",
    "Now we bring everything together:\n",
    "\n",
    "* Load the datasets and convert them to PyTorch tensors.\n",
    "* Create a DataLoader for batching and shuffling.\n",
    "* Initialize and train the CNN.\n",
    "* Save the trained model to disk.\n",
    "\"\"\"\n",
    "\n",
    "# Load datasets\n",
    "X, y, label_mapping = load_coin_datasets('coin_data')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "# model = CoinClassifier(label_mapping=label_mapping)\n",
    "model = CoinClassifierEfficientNet(label_mapping=label_mapping)\n",
    "model.compile()\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataloader, num_epochs=20, log_interval=5)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"coin_classifier\")\n",
    "print(\"Training complete! Model saved as coin_classifier.zip\")"
   ],
   "id": "4313a6f5260c73ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 9: Load and Inspect the Model\n",
    "\n",
    "After training, you can reload the saved model and check its metadata.\n",
    "\"\"\"\n",
    "\n",
    "session = ModelLoader.from_path(\"coin_classifier.zip\")\n",
    "\n",
    "print(f'Model type: {session.model_type}')\n",
    "print(f'Description: {session.description}')\n",
    "print(f'Label mapping: {session.label_mapping}')"
   ],
   "id": "e5fd5d30bc166e93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 10: Predict from a Single Image\n",
    "\n",
    "We now define a helper function to classify one image with the trained model.\n",
    "\n",
    "* The image is expanded with a batch dimension before inference.\n",
    "* The ONNX session returns model outputs.\n",
    "* The highest-scoring class is mapped back to its label.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "def predict_coin(image: np.ndarray, session: ort.InferenceSession) -> str:\n",
    "    \"\"\"Predict coin type from a single image.\"\"\"\n",
    "    # Add batch dimension and run inference\n",
    "    input_batch = np.expand_dims(image, axis=0)\n",
    "    output = session.run(['output'], {'input': input_batch})[0]\n",
    "\n",
    "    # Get prediction and convert to label\n",
    "    predicted_class = np.argmax(output[0])\n",
    "    predicted_label = session.label_mapping[str(predicted_class)]\n",
    "\n",
    "    return predicted_label"
   ],
   "id": "219ee7f322a4aed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 11: Test the Model\n",
    "\n",
    "Finally, let’s check the trained model on a few random images.\n",
    "\n",
    "* Pick random samples from the dataset.\n",
    "* Run predictions with `predict_coin`.\n",
    "* Compare predicted vs. true labels.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(20):\n",
    "    idx = random.randint(0, len(X) - 1)\n",
    "    test_image = X[idx]\n",
    "    true_label = label_mapping[y[idx]]\n",
    "\n",
    "    predicted_label = predict_coin(test_image, session)\n",
    "    print(f\"True label: {true_label} | Predicted label: {predicted_label} | {true_label == predicted_label}\")"
   ],
   "id": "667187037a2454ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 12: Evaluate Accuracy\n",
    "\n",
    "Instead of just printing individual results,\n",
    "we can calculate overall accuracy across 100 random images.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "num_tests = 100\n",
    "correct = 0\n",
    "\n",
    "for i in range(num_tests):\n",
    "    idx = random.randint(0, len(X) - 1)\n",
    "    test_image = X[idx]\n",
    "    true_label = label_mapping[y[idx]]\n",
    "    predicted_label = predict_coin(test_image, session)\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / num_tests\n",
    "print(f\"Accuracy over {num_tests} random samples: {accuracy:.2%}\")"
   ],
   "id": "901324ea379d9cf3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
