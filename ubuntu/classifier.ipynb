{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Step 1: Collect and Load Data\n",
    "\n",
    "Use the OpenTouch Interface to record your dataset.\n",
    "\n",
    "* Make one recording per data class (e.g., one for each type of coin).\n",
    "* Each `.touch` file should contain data for only one label/class.\n",
    "* You’ll need to repeat the loading and preprocessing steps for all .touch files (one per label) before training.\n",
    "\n",
    "Tip: Seeing a warning from Streamlit is normal and not an error.\n",
    "\"\"\"\n",
    "\n",
    "from opentouch_interface.decoder import Decoder\n",
    "\n",
    "# Replace this with the full path to one of your dataset files\n",
    "# Example: \"/home/username/datasets/coin1.touch\"\n",
    "path = ...\n",
    "\n",
    "# Load the dataset for this label\n",
    "dataset = Decoder(path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 2: Inspect the Raw Data\n",
    "\n",
    "Now let’s check the structure of your recorded dataset.\n",
    "\n",
    "* `dataset.sensor_names` (list[str]) lists all sensors that were captured.\n",
    "* `dataset.stream_names_of(sensor_name)` (list[str]) lists the streams for a given sensor\n",
    "  (for DIGIT this will just be \"camera\").\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Inspect the raw data by printing the stream names for each sensor\n"
   ],
   "id": "91beebec50b740ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 3: Grab the Camera Frames\n",
    "\n",
    "The raw dataset contains both the sensor data and additional metadata\n",
    "(e.g., timestamps). For training we only need the actual frames.\n",
    "\n",
    "* `dataset.stream_data_of(sensor_name, stream_name)` returns the list of frames.\n",
    "* For DIGIT, the stream is `\"camera\"`, which gives you the captured images.\n",
    "\n",
    "Hint: Call `dataset.stream_data_of` with `with_delta=False`.\n",
    "\"\"\"\n",
    "# TODO: Extract the camera frames from the raw data\n",
    "camera_data: list = [...]\n"
   ],
   "id": "8292e4266d5f30ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 4: Filter the Frames\n",
    "\n",
    "Each dataset should only contain images of its respective label.\n",
    "Remove frames that don’t match (e.g., \"no touch\" images in a \"coin\" dataset).\n",
    "\n",
    "Why?\n",
    "The raw data also includes unwanted frames (like empty touches or noise).\n",
    "Filtering ensures that each dataset is clean and only contains the intended label.\n",
    "\n",
    "* Exception: If you are creating a \"no touch\" dataset, keep the empty frames.\n",
    "* Hint: You can do this both programmatically and using your file explorer.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Filter the frames to only include frames of the correct label\n",
    "with_touch: list = [...]\n"
   ],
   "id": "d88b3a00398c1038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 5: Save the Cleaned Dataset\n",
    "\n",
    "Now save the filtered frames to disk.\n",
    "\n",
    "* Saving them as `.png` files makes it easy to inspect the images in your file explorer.\n",
    "* Each dataset (per label) will be stored in its own folder.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Use the .touch filename (without extension) as the dataset name\n",
    "dset_name = os.path.splitext(os.path.basename(path))[0]\n",
    "directory = os.path.join(\"coin_data\", dset_name)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Saving {len(with_touch)} images to {directory}/\")\n",
    "for i, frame in enumerate(with_touch):\n",
    "    img = Image.fromarray(frame.astype(np.uint8))\n",
    "    img.save(os.path.join(directory, f\"{dset_name}_{i:04d}.png\"))\n"
   ],
   "id": "7b07b505a87336cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 6: Define the CNN Model\n",
    "\n",
    "We now build a Convolutional Neural Network (CNN) to classify the coins.\n",
    "\n",
    "* The model takes RGB frames as input (3 channels).\n",
    "* It outputs one class per label in your dataset.\n",
    "* Preprocessing converts images from [N, H, W, C] to [N, C, H, W]\n",
    "  and normalizes pixel values to [0, 1].\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Dict, Any\n",
    "\n",
    "from opentouch.core.base_cnn import BaseCNN\n",
    "\n",
    "\n",
    "class CoinClassifier(BaseCNN):\n",
    "    def __init__(self, label_mapping: dict) -> None:\n",
    "        super().__init__(input_channels=3, label_mapping=label_mapping)\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        labels = \", \".join(self.label_mapping.values())\n",
    "        return f\"A CNN classifier for distinguishing between: {labels}\"\n",
    "\n",
    "    def build(self) -> None:\n",
    "        \"\"\"\n",
    "        Define CNN architecture for coin classification.\n",
    "\n",
    "        e.g., self.model = nn.Sequential(...)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Define the CNN architecture\n",
    "\n",
    "        self.model = ...\n",
    "\n",
    "    def preprocess(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Convert [N, H, W, C] → [N, C, H, W], normalize to [0, 1].\n",
    "\n",
    "        Don't modify this part.\n",
    "        \"\"\"\n",
    "        x = x.float() / 255.0\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "    def onnx_export(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parameters for ONNX export.\n",
    "\n",
    "        Don't modify this part if you use a standard DIGIT frame as your model input.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"example_input\": torch.randint(0, 256, (1, 320, 240, 3), dtype=torch.uint8),\n",
    "            \"input_names\": [\"input\"],\n",
    "            \"output_names\": [\"output\"],\n",
    "        }"
   ],
   "id": "98c4b94e1c28063b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 7: Load the Coin Datasets\n",
    "\n",
    "Now we load the saved images back into memory.\n",
    "\n",
    "* Each coin type should be in its own subdirectory under `coin_data/`.\n",
    "* A label mapping is created automatically from the folder names.\n",
    "* Optionally, datasets are balanced so all classes have the same number of images.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_coin_datasets(dset_path: str, balance_datasets: bool = True, max_size=2000) -> tuple[np.ndarray, np.ndarray, dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load coin images from subdirectories.\n",
    "\n",
    "    Expected structure:\n",
    "    coin_data/\n",
    "    ├── two_euro/\n",
    "    │   ├── two_euro_0001.png\n",
    "    │   └── ...\n",
    "    ├── one_euro/\n",
    "    │   ├── one_euro_0001.png\n",
    "    │   └── ...\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(dset_path):\n",
    "        raise FileNotFoundError(f\"Directory '{dset_path}' does not exist.\")\n",
    "\n",
    "    coin_dirs = [d for d in os.listdir(dset_path) if os.path.isdir(os.path.join(dset_path, d))]\n",
    "    coin_dirs.sort()\n",
    "    if not coin_dirs:\n",
    "        raise ValueError(f\"No subdirectories found in '{dset_path}'.\")\n",
    "\n",
    "    # Create label mapping\n",
    "    labels = {j: coin_name for j, coin_name in enumerate(coin_dirs)}\n",
    "    print(f\"Found {len(coin_dirs)} coin types:\")\n",
    "    for label, name in labels.items():\n",
    "        print(f\"  Label {label}: {name}\")\n",
    "\n",
    "    # Load images\n",
    "    coin_images = {}\n",
    "    for label, coin_name in labels.items():\n",
    "        coin_path = os.path.join(dset_path, coin_name)\n",
    "        image_files = sorted([f for f in os.listdir(coin_path) if f.lower().endswith(\".png\")])\n",
    "\n",
    "        images = [np.array(Image.open(os.path.join(coin_path, f))) for f in image_files]\n",
    "        coin_images[label] = images\n",
    "\n",
    "    # Balance datasets (optional)\n",
    "    all_images, all_labels = [], []\n",
    "    if balance_datasets:\n",
    "        smallest_dset = min(min(len(images) for images in coin_images.values()), max_size)\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images[:smallest_dset])\n",
    "            all_labels.extend([label] * smallest_dset)\n",
    "    else:\n",
    "        for label, images in coin_images.items():\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend([label] * len(images))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.stack(all_images, axis=0)  # Shape: (N, H, W, C)\n",
    "    Y = np.array(all_labels)          # Shape: (N,)\n",
    "\n",
    "    return X, Y, labels\n"
   ],
   "id": "f958e910ee30b60a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 8: Train and Save the Model\n",
    "\n",
    "Now we bring everything together:\n",
    "\n",
    "* Load the datasets and convert them to PyTorch tensors.\n",
    "* Create a DataLoader for batching and shuffling.\n",
    "* Initialize and train the CNN.\n",
    "* Save the trained model to disk.\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# TODO: Load datasets with `load_coin_datasets(\"coin_data\")\n",
    "\n",
    "# TODO: Convert the images and labels to PyTorch tensors\n",
    "X_tensor = ...\n",
    "y_tensor = ...\n",
    "\n",
    "# TODO: Wrap the tensors in a TensorDataset and create a DataLoader\n",
    "dataset = ...\n",
    "dataloader = ...  # Select a batch size that fits your GPU memory\n",
    "\n",
    "# TODO: Initialize and compile the model\n",
    "model = ...\n",
    "model.compile()\n",
    "\n",
    "# TODO: Train the model using `model.fit(...)`. Lookup the needed arguments\n",
    "\n",
    "# TODO: Save the model using `model.save(...)`.\n"
   ],
   "id": "4313a6f5260c73ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 9: Load and Inspect the Model\n",
    "\n",
    "After training, you can reload the saved model and check its metadata.\n",
    "\"\"\"\n",
    "\n",
    "from opentouch.core.model_loader import ModelLoader\n",
    "\n",
    "# TODO: Load the model using `ModelLoader.from_path('<model_name>.zip')`\n",
    "session = ...\n",
    "\n",
    "print(f\"Model type: {session.model_type}\")\n",
    "print(f\"Description: {session.description}\")\n",
    "print(f\"Label mapping: {session.label_mapping}\")\n"
   ],
   "id": "e5fd5d30bc166e93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 10: Predict from a Single Image\n",
    "\n",
    "We now define a helper function to classify one image with the trained model.\n",
    "\n",
    "* The image is expanded with a batch dimension before inference.\n",
    "* The ONNX session returns model outputs.\n",
    "* The highest-scoring class is mapped back to its label.\n",
    "\n",
    "DON'T MODIFY THIS CELL. SIMPLY RUN.\n",
    "\"\"\"\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def predict_coin(image: np.ndarray, session: ort.InferenceSession) -> str:\n",
    "    \"\"\"Predict coin type from a single image.\"\"\"\n",
    "    # Add batch dimension [H, W, C] -> [1, H, W, C]\n",
    "    input_batch = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Run inference\n",
    "    output = session.run([\"output\"], {\"input\": input_batch})[0]\n",
    "\n",
    "    # Pick the class with highest score\n",
    "    predicted_class = int(np.argmax(output[0]))\n",
    "\n",
    "    # Convert index back to label name\n",
    "    predicted_label = session.label_mapping[str(predicted_class)]\n",
    "\n",
    "    return predicted_label"
   ],
   "id": "219ee7f322a4aed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 11: Test the Model\n",
    "\n",
    "Finally, let’s check the trained model on a few random images.\n",
    "\n",
    "* Pick random samples from the dataset.\n",
    "* Run predictions with `predict_coin`.\n",
    "* Compare predicted vs. true labels.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(20):\n",
    "    idx = random.randint(...)\n",
    "    test_image = ...\n",
    "    true_label = ...\n",
    "    predicted_label = ...\n",
    "\n",
    "    print(f\"True: {true_label} | Predicted: {predicted_label} | Correct: {true_label == predicted_label}\")\n"
   ],
   "id": "667187037a2454ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 12: Evaluate Accuracy\n",
    "\n",
    "Instead of just printing individual results,\n",
    "we can calculate overall accuracy across 100 random images.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "num_tests = 100\n",
    "correct = 0\n",
    "\n",
    "for i in range(num_tests):\n",
    "    idx = random.randint(...)\n",
    "    test_image = ...\n",
    "    true_label = ...\n",
    "    predicted_label = predict_coin(test_image, session)\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / num_tests\n",
    "print(f\"Accuracy over {num_tests} random samples: {accuracy:.2%}\")\n"
   ],
   "id": "4d50141f1d69547d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
